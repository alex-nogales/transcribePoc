{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "governing-appliance",
   "metadata": {},
   "source": [
    "# Score Transcriptions with IPA and no IPA\n",
    "### Install python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "golden-staff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (20.3.3)\n",
      "Collecting pip\n",
      "  Using cached pip-21.0.1-py3-none-any.whl (1.5 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 20.3.3\n",
      "    Uninstalling pip-20.3.3:\n",
      "      Successfully uninstalled pip-20.3.3\n",
      "Successfully installed pip-21.0.1\n",
      "Requirement already satisfied: Unidecode in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (1.2.0)\n",
      "Requirement already satisfied: python-Levenshtein in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (0.12.2)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from python-Levenshtein) (51.1.2)\n"
     ]
    }
   ],
   "source": [
    "!/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip\n",
    "!pip install Unidecode\n",
    "!pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collected-bachelor",
   "metadata": {},
   "source": [
    "# Import Pyhthon Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "detailed-specialist",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import boto3\n",
    "import sys, os\n",
    "from datetime import date, timedelta, datetime, timezone\n",
    "import time \n",
    "import numpy as np\n",
    "import random\n",
    "import tarfile\n",
    "import unidecode as uni\n",
    "import re\n",
    "import Levenshtein as lv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisite-basic",
   "metadata": {},
   "source": [
    "# Define functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proud-qatar",
   "metadata": {},
   "source": [
    "### Function to get file name from S3 buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "regular-distinction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_s3_objects(s3, **base_kwargs):\n",
    "    ###\n",
    "    #   Function to amplify the limit of AWS results to 1000+\n",
    "    ###\n",
    "    continuation_token = None\n",
    "    while True:\n",
    "        list_kwargs = dict(MaxKeys=1000, **base_kwargs)\n",
    "        if continuation_token:\n",
    "            list_kwargs['ContinuationToken'] = continuation_token\n",
    "        response = s3.list_objects_v2(**list_kwargs)\n",
    "        yield from response.get('Contents', [])\n",
    "        if not response.get('IsTruncated'):  # At the end of the list?\n",
    "            break\n",
    "        continuation_token = response.get('NextContinuationToken')\n",
    "\n",
    "def get_folder_list(bucket='awstranscribe-tests', key='transcribeOutputs/Files'):\n",
    "    ###\n",
    "    #  Get the name of the files in a bucket. While bucket is the AWS S3 Bucket and key is the folder inside that bucket\n",
    "    # it defaults to transcribeOutputs/Files\n",
    "    ###\n",
    "    s3 = boto3.client('s3')\n",
    "    data_loc = []\n",
    "    for obj in get_all_s3_objects(s3, Bucket=bucket, Prefix=key):\n",
    "        names = 's3://{}/{}'.format(bucket, obj['Key'])\n",
    "        data_loc.append(names)\n",
    "    return data_loc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convenient-microwave",
   "metadata": {},
   "source": [
    "### Function to create a .csv file from a Pandas DataFrame\n",
    "Creates a file into a definded AWS S3 folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "classical-softball",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_file(df, file_name=date.today(), key='transcribeOutputs/proc_files/'):\n",
    "    today = date.today()\n",
    "    csv_buffer = StringIO()\n",
    "    data_frame = df\n",
    "    data_frame.to_csv(csv_buffer, decimal='.', sep=',', encoding='utf-8', index=False, header=None)\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    s3_resource.Object('awstranscribe-tests', f'{key}{file_name}.csv').put(Body=csv_buffer.getvalue()) ## CHANGE temp_Mail for Mails\n",
    "    \n",
    "    return f'Saved as file: awstranscribe-tests/{key}{file_name}.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opponent-netscape",
   "metadata": {},
   "source": [
    "### Function to neutralize words\n",
    "Remove accent characters and lowercase the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "coupled-timer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neutralize(a_string):\n",
    "    a_string = uni.unidecode(a_string)\n",
    "    a_string = re.sub('[?!@#$.,]', '', a_string)\n",
    "    return a_string.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunset-bleeding",
   "metadata": {},
   "source": [
    "### function that calculates percentage of similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "hearing-harvard",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_score(string_real, string_model, extra_info=False):\n",
    "    # cut strings to bag of words\n",
    "    words_real = neutralize(string_real).split(' ')\n",
    "    count = 0\n",
    "    error_words = []\n",
    "    for word in words_real:\n",
    "        # search word in string_model\n",
    "        if word in neutralize(string_model):\n",
    "            count = count + 1\n",
    "        else:\n",
    "            error_words.append(word)\n",
    "\n",
    "    score = count / len(words_real)\n",
    "    if extra_info:\n",
    "        return score, error_words\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocal-strap",
   "metadata": {},
   "source": [
    "### Function that calculates the percentage of similarity words usaing DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dedicated-greene",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_score_dataframes(_df_real, _df_model):\n",
    "    score_list = []\n",
    "\n",
    "    for a_file, a_string in zip(_df_model['file'], _df_model['transcript']):\n",
    "        # first, we search for file in real\n",
    "        real_str = _df_real[_df_real['file']==a_file]['transcript']\n",
    "\n",
    "        # files should be 1:1, if not, we riot\n",
    "        if len(real_str) != 1:f\n",
    "            AttributeError('Dude files are not 1:1 in ' + a_file)\n",
    "\n",
    "        # otherwise let's continue calculating the score\n",
    "        #print(real_str.array)\n",
    "        score, fails = similarity_score(real_str.array[0], a_string, extra_info=True)\n",
    "        score_list.append((a_file, score, fails))\n",
    "\n",
    "    return pd.DataFrame(score_list, columns=['file', 'score', 'failed_words'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attended-county",
   "metadata": {},
   "source": [
    "### Read JSON from transcribe output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "advisory-valuable",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Saved as file: awstranscribe-tests/levenshteinTests/consolidados/ipa.csv'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_transcript(data_location):\n",
    "    data_loc = get_folder_list(key=data_location)\n",
    "    container = []\n",
    "    for file in data_loc[1:]:\n",
    "        data = pd.read_json(file)\n",
    "        fname = os.path.basename(file)\n",
    "        results = data['results'].get('transcripts')[0].get('transcript')\n",
    "        tp = (fname, results)\n",
    "        container.append(tp)\n",
    "    df_transcripts = pd.DataFrame(container, columns = ['fname', 'transcript'])\n",
    "    return df_transcripts\n",
    "\n",
    "raw = extract_transcript('levenshteinTests/RAW/')\n",
    "ipa = extract_transcript('levenshteinTests/IPA/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "according-bangkok",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lv_score(a_series, b_series):\n",
    "    metric = 0\n",
    "    for a_string, b_string in zip(a_series, b_series):\n",
    "        a_string = neutralize(a_string)\n",
    "        b_string = neutralize(b_string)\n",
    "        metric = lv.distance(a_string, b_string) / len(a_string)\n",
    "    return 1 - metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "second-danish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-13.6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nmetrica = 0\\nfor ipa_word, raw_word in zip(ipa_words, raw_words):\\n    ipa = neutralize(ipa_word)\\n    raw = neutralize(raw_word)\\n    metrica = lv.distance(ipa, raw) / len(ipa)\\nprint(1 - metrica)\\n'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for word in get_folder_list(key='levenshteinTests/consolidados/')[1:]:\n",
    "    if 'ipa' in word:\n",
    "        ipa_words = pd.read_csv(word, names=['file', 'transcript'])\n",
    "    if 'raw' in word:\n",
    "        raw_words = pd.read_csv(word, names=['file', 'transcript'])\n",
    "    if 'real' in word: \n",
    "        real_words = pd.read_csv(word, names=['transcript'])\n",
    " \n",
    "\n",
    "ipa_words = ipa_words['transcript']\n",
    "raw_words = raw_words['transcript']\n",
    "\n",
    "print(lv_score(real_words, raw_words))\n",
    "\n",
    "'''\n",
    "metrica = 0\n",
    "for ipa_word, raw_word in zip(ipa_words, raw_words):\n",
    "    ipa = neutralize(ipa_word)\n",
    "    raw = neutralize(raw_word)\n",
    "    metrica = lv.distance(ipa, raw) / len(ipa)\n",
    "print(1 - metrica)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaged-circus",
   "metadata": {},
   "source": [
    "# Testing executions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "actual-transcript",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9814814814814815\n"
     ]
    }
   ],
   "source": [
    "df_real = get_folder_list(key='transcribeOutputs/proc_files/IPA')\n",
    "df_model = get_folder_list(key='transcribeOutputs/proc_files/no_IPA')\n",
    "\n",
    "for df_real_ in df_real[1:]:\n",
    "    df_real_1 = pd.read_csv(df_real_, names=['file', 'transcript'])\n",
    "\n",
    "for df_model_ in df_model[1:]:\n",
    "    df_model_1 = pd.read_csv(df_model_, names=['file', 'transcript'])\n",
    "\n",
    "#scr_list = similarity_score_dataframes(df_real_1, df_model_1)\n",
    "#print(\"Mean: \", scr_list['score'].mean())\n",
    "#print(scr_list.head()) \n",
    "\n",
    "words_real = df_real_1['transcript']\n",
    "words_test = df_model_1['transcript']\n",
    "\n",
    "metrica = 0\n",
    "for word_real, word_test in zip(words_real, words_test):\n",
    "    good = neutralize(word_real)\n",
    "    bad = neutralize(word_test)\n",
    "    metrica = lv.distance(good, bad)  / len(good)\n",
    "print(1 - metrica)\n",
    "#to_file(scr_list, file_name='Testing01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "contrary-plaintiff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alo buenas tardes hablo con don jorge cadenas kilos favor diga si o no alo no se debemos disculpe pero usted don jorge cadenas quiero\n",
      "a lo buenas tardes hablo con don jorge cadenas kilos favor diga si o no a no se debemos disculpe pero usted don jorge cadenas quiero\n",
      "133\n",
      "0.9774436090225564\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nice = 'alo? Buenas tardes. Habló con Don Jorge. Cadenas, kilos favor Diga sí o no. alo? No se debemos. Disculpe. Pero usted, Don Jorge Cadenas, quiero'\n",
    "bad = 'a lo buenas tardes. Habló con Don Jorge. Cadenas, kilos Favor, Diga sí o no. a No se debemos. Disculpe. Pero usted, Don Jorge Cadenas, quiero'\n",
    "nice = neutralize(nice)\n",
    "bad = neutralize(bad)\n",
    "\n",
    "print(nice)\n",
    "print(bad)\n",
    "## Lev / lawrgo de palabra  \n",
    "print(len(nice))\n",
    "metrica = lv.distance(nice, bad)  / len(nice)\n",
    "print(1 - metrica)\n",
    "lv.distance('aaa', 'aaa')   / len('aaa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mounted-concept",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
